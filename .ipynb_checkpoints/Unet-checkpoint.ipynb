{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c491e-0827-431d-b0d3-2fcae35cac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9278ef1-0b65-4e00-b0ae-f87e3e7c2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_display_images(gt_folder, img_folder):\n",
    "    gt_files = sorted(os.listdir(gt_folder))\n",
    "    img_files = sorted(os.listdir(img_folder))\n",
    "    \n",
    "    for i, (gt_file, img_file) in enumerate(zip(gt_files, img_files)):\n",
    "        if i >= 50:\n",
    "            break\n",
    "        gt_path = os.path.join(gt_folder, gt_file)\n",
    "        img_path = os.path.join(img_folder, img_file)\n",
    "        \n",
    "        gt_image = cv2.imread(gt_path)\n",
    "        img_image = cv2.imread(img_path)\n",
    "        \n",
    "        if gt_image is None or img_image is None:\n",
    "            print(f\"Error loading {gt_file} or {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Resize images to the same size\n",
    "        gt_image = cv2.resize(gt_image, (img_image.shape[1], img_image.shape[0]))\n",
    "        \n",
    "        # Display images side by side\n",
    "        plt.figure(figsize=(10,5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(img_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image: {img_file}\")\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"GT: {gt_file}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Define paths\n",
    "gt_path = os.path.join(\"./GT\")\n",
    "img_path = os.path.join(\"./Images\")\n",
    "\n",
    "# Run the function\n",
    "load_and_display_images(gt_path, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7c7fe-4d4e-4885-b060-885bacf6b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.8149, Val Loss: 0.6931\n",
      "Epoch 2/10, Train Loss: 0.6931, Val Loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, gt_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.gt_paths = gt_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_paths[idx])\n",
    "        gt = cv2.imread(self.gt_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        gt = cv2.resize(gt, (256, 256))\n",
    "        \n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        gt = gt.astype(np.float32) / 255.0\n",
    "        \n",
    "        img = torch.tensor(img).permute(2, 0, 1)  # Convert to (C, H, W) format\n",
    "        gt = torch.tensor(gt).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        return img, gt\n",
    "    \n",
    "    @classmethod\n",
    "    def from_folders(cls, gt_folder, img_folder, transform=None):\n",
    "        gt_files = sorted(os.listdir(gt_folder))\n",
    "        img_files = sorted(os.listdir(img_folder))\n",
    "        \n",
    "        gt_paths = [os.path.join(gt_folder, f) for f in gt_files]\n",
    "        img_paths = [os.path.join(img_folder, f) for f in img_files]\n",
    "        \n",
    "        return cls(img_paths, gt_paths, transform)\n",
    "\n",
    "# Define U-Net Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.encoder1 = conv_block(3, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        \n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "        \n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = self.decoder4(torch.cat((dec4, enc4), dim=1))\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = self.decoder3(torch.cat((dec3, enc3), dim=1))\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = self.decoder2(torch.cat((dec2, enc2), dim=1))\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.decoder1(torch.cat((dec1, enc1), dim=1))\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(dec1))\n",
    "    \n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Plot Training and Validation Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset.from_folders(\"./GT\", \"./Images\")\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "model = UNet()\n",
    "train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb58330-f537-46dd-8e3a-c1dc91f7b449",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 171) (2985089610.py, line 171)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 171\u001b[1;36m\u001b[0m\n\u001b[1;33m    if __name__ == \"__main:\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 171)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, gt_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.gt_paths = gt_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_paths[idx])\n",
    "        gt = cv2.imread(self.gt_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        gt = cv2.resize(gt, (256, 256))\n",
    "        \n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        gt = gt.astype(np.float32) / 255.0\n",
    "        \n",
    "        img = torch.tensor(img).permute(2, 0, 1)  # Convert to (C, H, W) format\n",
    "        gt = torch.tensor(gt).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        return img, gt\n",
    "    \n",
    "    @classmethod\n",
    "    def from_folders(cls, gt_folder, img_folder, transform=None):\n",
    "        gt_files = sorted(os.listdir(gt_folder))\n",
    "        img_files = sorted(os.listdir(img_folder))\n",
    "        \n",
    "        gt_paths = [os.path.join(gt_folder, f) for f in gt_files]\n",
    "        img_paths = [os.path.join(img_folder, f) for f in img_files]\n",
    "        \n",
    "        return cls(img_paths, gt_paths, transform)\n",
    "\n",
    "# Define U-Net Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.encoder1 = conv_block(3, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        \n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "        \n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = self.decoder4(torch.cat((dec4, enc4), dim=1))\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = self.decoder3(torch.cat((dec3, enc3), dim=1))\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = self.decoder2(torch.cat((dec2, enc2), dim=1))\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.decoder1(torch.cat((dec1, enc1), dim=1))\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(dec1))\n",
    "\n",
    "# Save model function\n",
    "def save_model(model, path=\"unet_model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Testing function\n",
    "def test_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            \n",
    "            tp = (preds * masks).sum().item()\n",
    "            tn = ((1 - preds) * (1 - masks)).sum().item()\n",
    "            fp = (preds * (1 - masks)).sum().item()\n",
    "            fn = ((1 - preds) * masks).sum().item()\n",
    "            \n",
    "            dice = (2 * tp) / (2 * tp + fp + fn + 1e-7)\n",
    "            iou = tp / (tp + fp + fn + 1e-7)\n",
    "            sensitivity = tp / (tp + fn + 1e-7)\n",
    "            specificity = tn / (tn + fp + 1e-7)\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-7)\n",
    "            \n",
    "            dice_scores.append(dice)\n",
    "            iou_scores.append(iou)\n",
    "            sensitivities.append(sensitivity)\n",
    "            specificities.append(specificity)\n",
    "            accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Dice Coefficient: {np.mean(dice_scores):.4f}\")\n",
    "    print(f\"IoU Score: {np.mean(iou_scores):.4f}\")\n",
    "    print(f\"Sensitivity: {np.mean(sensitivities):.4f}\")\n",
    "    print(f\"Specificity: {np.mean(specificities):.4f}\")\n",
    "    print(f\"Pixel-wise Accuracy: {np.mean(accuracies):.4f}\")\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df22278-2381-486f-a93b-88f785f9fdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db9f01-7a57-4875-a163-3ab2538cf3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
